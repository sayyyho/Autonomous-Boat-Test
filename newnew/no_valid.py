"""
KABOAT YOLO í›ˆë ¨ - data.yaml ìë™ ìˆ˜ì •
valid ê²½ë¡œ ë¬¸ì œ í•´ê²°
"""

from ultralytics import YOLO
import yaml
import os
from pathlib import Path
import torch
import shutil


def detect_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
    if torch.cuda.is_available():
        device = '0'
        device_name = f"GPU ({torch.cuda.get_device_name(0)})"
        print(f"âœ… GPU ê°ì§€: {device_name}")
    else:
        device = 'cpu'
        device_name = "CPU"
        print(f"âš ï¸  GPU ì—†ìŒ. CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    return device, device_name


def fix_data_yaml(dataset_path: str):
    """data.yaml íŒŒì¼ ìë™ ìˆ˜ì •"""
    dataset_path = Path(dataset_path)
    yaml_path = dataset_path / 'data.yaml'
    
    if not yaml_path.exists():
        print(f"âŒ data.yamlì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
        return False
    
    # ë°±ì—…
    backup_path = dataset_path / 'data.yaml.backup'
    if not backup_path.exists():
        shutil.copy(yaml_path, backup_path)
        print(f"ğŸ’¾ ë°±ì—… ìƒì„±: {backup_path}")
    
    # ì½ê¸°
    with open(yaml_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # valid í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    has_valid_folder = (dataset_path / 'valid').exists()
    
    modified = False
    
    # â˜…â˜…â˜… ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤ â˜…â˜…â˜…
    # valid í´ë”ê°€ ì—†ê³ , data.yamlì— 'val' í‚¤ë„ ì—†ìœ¼ë©´
    if not has_valid_folder and 'val' not in config:
        print("ğŸ”§ data.yaml ìˆ˜ì • ì¤‘...")
        # 'val' í‚¤ë¥¼ 'train'ê³¼ ë™ì¼í•˜ê²Œ *ì¶”ê°€*í•©ë‹ˆë‹¤.
        # (YOLO ë¡œë”ë¥¼ í†µê³¼ì‹œí‚¤ê¸° ìœ„í•œ íŠ¸ë¦­)
        config['val'] = config['train'] 
        modified = True
        print(f"   After: 'val: {config['train']}' ì¶”ê°€ (ìë™ ë¶„í•  ì˜ˆì •)")

    # (ì°¸ê³ ) ë§Œì•½ 'test' í‚¤ê°€ ì—†ë‹¤ë©´ 'val'ê³¼ ë™ì¼í•˜ê²Œ ì¶”ê°€
    if 'test' not in config:
        config['test'] = config['val']
        modified = True
        print(f"   After: 'test: {config['val']}' ì¶”ê°€")
    # â˜…â˜…â˜… ì—¬ê¸°ê¹Œì§€ â˜…â˜…â˜…

    # ì €ì¥
    if modified:
        with open(yaml_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)
        print(f"âœ… data.yaml ìˆ˜ì • ì™„ë£Œ")
    else:
        print(f"âœ… data.yaml ìˆ˜ì • ë¶ˆí•„ìš”")
    
    return True

def check_dataset_structure(dataset_path: str):
    """ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦"""
    dataset_path = Path(dataset_path)
    
    print("=" * 60)
    print("ğŸ“ ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸")
    print("=" * 60)
    
    # í•„ìˆ˜ íŒŒì¼/í´ë”
    required_items = {
        'data.yaml': dataset_path / 'data.yaml',
        'train': dataset_path / 'train',
    }
    
    # ì„ íƒ í•­ëª©
    optional_items = {
        'valid': dataset_path / 'valid',
        'test': dataset_path / 'test',
    }
    
    # í•„ìˆ˜ ì²´í¬
    all_exist = True
    for name, path in required_items.items():
        exists = path.exists()
        status = "âœ…" if exists else "âŒ"
        print(f"{status} {name}: {path}")
        all_exist = all_exist and exists
    
    # ì„ íƒ ì²´í¬
    for name, path in optional_items.items():
        exists = path.exists()
        status = "âœ…" if exists else "âšª"
        print(f"{status} {name}: {path} (ì„ íƒ)")
    
    if not all_exist:
        print("\nâš ï¸  í•„ìˆ˜ íŒŒì¼/í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    # valid ì—†ìœ¼ë©´ ê²½ê³ 
    has_valid = (dataset_path / 'valid').exists()
    if not has_valid:
        print("\nğŸ’¡ valid í´ë” ì—†ìŒ: trainì˜ ì¼ë¶€ë¥¼ ìë™ ë¶„í• ")
    
    # ë°ì´í„° ê°œìˆ˜
    try:
        train_images = list((dataset_path / 'train' / 'images').glob('*.jpg')) + \
                      list((dataset_path / 'train' / 'images').glob('*.png'))
        
        print(f"\nğŸ“Š ë°ì´í„° ê°œìˆ˜:")
        print(f"   Train: {len(train_images)} ì´ë¯¸ì§€")
        
        if has_valid:
            valid_images = list((dataset_path / 'valid' / 'images').glob('*.jpg')) + \
                          list((dataset_path / 'valid' / 'images').glob('*.png'))
            print(f"   Valid: {len(valid_images)} ì´ë¯¸ì§€")
            print(f"   Total: {len(train_images) + len(valid_images)} ì´ë¯¸ì§€")
        else:
            expected_train = int(len(train_images) * 0.8)
            expected_valid = len(train_images) - expected_train
            print(f"   â†’ í›ˆë ¨ìš©: ì•½ {expected_train} (80%)")
            print(f"   â†’ ê²€ì¦ìš©: ì•½ {expected_valid} (20%)")
        
    except Exception as e:
        print(f"\nâš ï¸  ë°ì´í„° í™•ì¸ ì˜¤ë¥˜: {e}")
    
    # data.yaml ë‚´ìš©
    try:
        with open(dataset_path / 'data.yaml', 'r') as f:
            config = yaml.safe_load(f)
            print(f"\nğŸ“‹ data.yaml:")
            print(f"   í´ë˜ìŠ¤ ìˆ˜: {config.get('nc', 'N/A')}")
            print(f"   í´ë˜ìŠ¤ ì´ë¦„: {config.get('names', 'N/A')}")
    except Exception as e:
        print(f"\nâš ï¸  data.yaml ì½ê¸° ì˜¤ë¥˜: {e}")
    
    print("=" * 60)
    
    # data.yaml ìë™ ìˆ˜ì •
    if not has_valid:
        fix_data_yaml(dataset_path)
    
    return True


def train_buoy_detector(
    dataset_path: str = './docking',
    model_size: str = 'n',
    epochs: int = 100,
    img_size: int = 640,
    batch_size: int = 16,
    project_name: str = 'kaboat_docking',
    device: str = 'auto',
    val_split: float = 0.2
):
    """ë¶€í‘œ ê²€ì¶œê¸° í›ˆë ¨"""
    
    # ë°ì´í„°ì…‹ ê²€ì¦ ë° data.yaml ìë™ ìˆ˜ì •
    if not check_dataset_structure(dataset_path):
        return None
    
    # ë””ë°”ì´ìŠ¤ ê°ì§€
    if device == 'auto':
        device, device_name = detect_device()
    else:
        device_name = device
    
    # CPU ë°°ì¹˜ ì¡°ì •
    if device == 'cpu' and batch_size > 8:
        original_batch = batch_size
        batch_size = 8
        print(f"\nâš ï¸  CPU: ë°°ì¹˜ {original_batch} â†’ {batch_size}")
    
    data_yaml = os.path.join(dataset_path, 'data.yaml')
    
    print("\n" + "=" * 60)
    print("ğŸš€ YOLO í›ˆë ¨ ì‹œì‘")
    print("=" * 60)
    print(f"ëª¨ë¸: YOLOv8{model_size}")
    print(f"ë°ì´í„°ì…‹: {dataset_path}")
    print(f"ì—í¬í¬: {epochs}")
    print(f"ë°°ì¹˜: {batch_size}")
    print(f"ë””ë°”ì´ìŠ¤: {device_name}")
    print("=" * 60)
    
    model = YOLO(f'yolov8{model_size}.pt')
    
    try:
        # valid ì—†ìœ¼ë©´ split í™œì„±í™”
        has_valid = Path(dataset_path, 'valid').exists()
        split_val = 0.0 if has_valid else val_split
        
        results = model.train(
            data=data_yaml,
            epochs=epochs,
            imgsz=img_size,
            batch=batch_size,
            name=project_name,
            device=device,
            split=split_val,
            patience=50,
            save=True,
            save_period=10,
            hsv_h=0.015,
            hsv_s=0.7,
            hsv_v=0.4,
            degrees=15,
            translate=0.1,
            scale=0.5,
            shear=0.0,
            perspective=0.0,
            flipud=0.0,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.0,
            optimizer='AdamW',
            lr0=0.01,
            lrf=0.01,
            momentum=0.937,
            weight_decay=0.0005,
            warmup_epochs=3,
            warmup_momentum=0.8,
            cos_lr=True,
            close_mosaic=10,
            verbose=True,
            seed=0,
            deterministic=True,
            workers=4 if device == 'cpu' else 8,
        )
        
        print("\n" + "=" * 60)
        print("âœ… í›ˆë ¨ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“ ê²°ê³¼: runs/detect/{project_name}/")
        print(f"ğŸ† best.pt: runs/detect/{project_name}/weights/best.pt")
        print("=" * 60)
        
        return results
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ì¤‘ë‹¨ë¨")
        return None
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        print("\nğŸ’¡ data.yaml.backupì—ì„œ ë³µì›:")
        print(f"   cp {dataset_path}/data.yaml.backup {dataset_path}/data.yaml")
        return None


def validate_trained_model(model_path: str, data_yaml: str):
    """ëª¨ë¸ ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ ê²€ì¦")
    print("=" * 60)
    
    try:
        model = YOLO(model_path)
        results = model.val(data=data_yaml)
        
        print(f"\nğŸ“ˆ ì„±ëŠ¥:")
        print(f"   mAP50: {results.box.map50:.3f}")
        print(f"   mAP50-95: {results.box.map:.3f}")
        print(f"   Precision: {results.box.p:.3f}")
        print(f"   Recall: {results.box.r:.3f}")
        print("=" * 60)
        
        return results
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return None


def test_on_image(model_path: str, image_path: str):
    """ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸"""
    import cv2
    
    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")
        return
    
    try:
        model = YOLO(model_path)
        results = model(image_path, conf=0.3)
        
        for r in results:
            print(f"\nğŸ¯ ê²€ì¶œ: {len(r.boxes)}ê°œ")
            for i, box in enumerate(r.boxes):
                cls_name = r.names[int(box.cls[0])]
                conf = float(box.conf[0])
                print(f"   {i+1}. {cls_name}: {conf:.3f}")
            
            img = r.plot()
            cv2.imwrite('result.jpg', img)
            print("\nğŸ’¾ ì €ì¥: result.jpg")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='./docking')
    parser.add_argument('--model', type=str, default='n', choices=['n', 's', 'm'])
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--img-size', type=int, default=640)
    parser.add_argument('--batch', type=int, default=16)
    parser.add_argument('--device', type=str, default='auto')
    parser.add_argument('--val-split', type=float, default=0.2)
    parser.add_argument('--mode', type=str, default='train', 
                       choices=['train', 'validate', 'test', 'check', 'fix'])
    parser.add_argument('--weights', type=str, default='runs/detect/kaboat_docking/weights/best.pt')
    parser.add_argument('--test-image', type=str, default='test.jpg')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì •ë³´
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ")
    print("=" * 60)
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA: {torch.cuda.is_available()}")
    print("=" * 60)
    
    # ì‹¤í–‰
    if args.mode == 'check':
        check_dataset_structure(args.dataset)
        
    elif args.mode == 'fix':
        # data.yamlë§Œ ìˆ˜ì •
        fix_data_yaml(args.dataset)
        
    elif args.mode == 'train':
        train_buoy_detector(
            dataset_path=args.dataset,
            model_size=args.model,
            epochs=args.epochs,
            img_size=args.img_size,
            batch_size=args.batch,
            device=args.device,
            val_split=args.val_split
        )
        
    elif args.mode == 'validate':
        data_yaml = os.path.join(args.dataset, 'data.yaml')
        validate_trained_model(args.weights, data_yaml)
        
    elif args.mode == 'test':
        test_on_image(args.weights, args.test_image)


if __name__ == '__main__':
    main()